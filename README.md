# ğŸš‘ Emergency Response Copilot (RAG + GenAI)

An AI-powered **Retrieval-Augmented Generation (RAG)** chatbot that provides real-time, accurate emergency response guidance by retrieving information from official SOP and legal PDF documents.

This system supports emergency scenarios such as **road accidents, fire incidents, floods, disaster management, and traffic law compliance**, generating structured step-by-step actions grounded in retrieved context.



## âœ¨ Key Features

- Emergency SOP-based Question Answering  
- Semantic Search over PDF Documents  
- Context-Grounded LLM Responses (Hallucination Controlled)  
- Step-by-Step Actionable Output Format  
- Streamlit Chatbot Interface  
- FastAPI Backend for Scalable Deployment  


## ğŸ—ï¸ System Architecture

<img width="1536" height="1023" alt="image" src="https://github.com/user-attachments/assets/d7185045-1d29-40f2-a978-70ce9a6f0667" />




## ğŸ“Œ Tech Stack

| Component        | Tool/Framework |
|-----------------|----------------|
| Frontend UI      | Streamlit |
| Backend API      | FastAPI |
| Vector Database  | ChromaDB |
| Embeddings       | HuggingFace MiniLM |
| LLM API          | Gemini / Groq |
| Document Source  | PDF SOP + Legal Docs |
| Deployment Ready | Render + Streamlit Cloud |


## ğŸ“‚ Project Structure

```bash
Emergency-Response-Copilot/
â”‚
â”œâ”€â”€ app.py                    # Streamlit Chatbot UI
â”œâ”€â”€ main.py                   # FastAPI Entry Point
â”œâ”€â”€ requirements.txt          # Dependencies
â”œâ”€â”€ README.md                 # Documentation
â”‚
â”œâ”€â”€ rag/
â”‚   â”œâ”€â”€ pipeline.py           # Full RAG workflow
â”‚   â”œâ”€â”€ retriever.py          # Chroma similarity search
â”‚   â”œâ”€â”€ llm.py                # LLM API integration
â”‚   â”œâ”€â”€ chunking.py           # Chunking logic
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ sop_docs.pdf          # Emergency SOP reference docs
â”‚
â””â”€â”€ chroma_db/                # Persistent vector store
```




## âš™ï¸ How It Works (RAG Pipeline)

1. **PDF SOP documents** are ingested and chunked  
2. Each chunk is converted into embeddings using **MiniLM**  
3. Chunks are stored in **Chroma Vector DB**  
4. User query triggers semantic retrieval of top-k matches  
5. Retrieved context is passed into **Gemini/Groq LLM**  
6. Model generates a grounded emergency response answer  


## ğŸš€ Installation & Setup

### 1ï¸âƒ£ Clone Repository

```bash
git clone https://github.com/your-username/Emergency-Response-Copilot-RAG.git
cd Emergency-Response-Copilot-RAG
```
2ï¸âƒ£ Create Virtual Environment
```bash
python -m venv venv
source venv/bin/activate   # Linux/Mac
venv\Scripts\activate      # Windows
```
3ï¸âƒ£ Install Dependencies
```bash
pip install -r requirements.txt
```
4ï¸âƒ£ Add API Key
```bash
Create a .env file in root directory:

GEMINI_API_KEY=your_api_key_here
```
â–¶ï¸ Run the Application
```
Start FastAPI Backend
uvicorn main:app --reload

Backend runs at:
http://127.0.0.1:8000

Start Streamlit Frontend
streamlit run app.py

App runs at:
http://localhost:8501
